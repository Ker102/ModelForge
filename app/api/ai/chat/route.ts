import { randomUUID } from "crypto"
import { NextResponse } from "next/server"
import { auth } from "@/lib/auth"
import { prisma } from "@/lib/db"
import type { Prisma } from "@prisma/client"
import {
  canConsumeAiRequest,
  getUsageSummary,
  logUsage,
} from "@/lib/usage"
import { streamLlmResponse, type LlmProviderSpec } from "@/lib/llm"
import type { GeminiMessage } from "@/lib/gemini"
import { createMcpClient } from "@/lib/mcp"
import { BlenderPlanner } from "@/lib/orchestration/planner"
import { PlanExecutor, type ExecutionResult } from "@/lib/orchestration/executor"
import type {
  ExecutionPlan,
  ExecutionLogEntry,
  PlanGenerationResult,
  PlanStep,
  PlanningMetadata,
  ResearchSource,
} from "@/lib/orchestration/types"
import { recordExecutionLog } from "@/lib/orchestration/monitor"
import { buildSystemPrompt } from "@/lib/orchestration/prompts"
import { searchFirecrawl, type FirecrawlSearchResult } from "@/lib/firecrawl"
import { classifyStrategy } from "@/lib/orchestration/strategy-router"
import { generateWorkflowProposal } from "@/lib/orchestration/workflow-advisor"
import { z } from "zod"

const MAX_HISTORY_MESSAGES = 12

type CommandStatus = "pending" | "ready" | "executed" | "failed"

interface CommandStub {
  id: string
  tool: string
  description: string
  status: CommandStatus
  confidence: number
  arguments: Record<string, unknown>
  notes?: string
}

interface ExecutedCommand extends CommandStub {
  status: "executed" | "failed"
  result?: unknown
  error?: string
}

function createStubId() {
  try {
    return randomUUID()
  } catch {
    return `stub-${Date.now()}`
  }
}

const WEB_RESEARCH_PATTERN = /(reference|research|inspiration|latest|trend|real[-\s]?world|accurate details|current|examples|ideas|design ideas|styles? from)/i

function shouldUseWebResearch(message: string) {
  return WEB_RESEARCH_PATTERN.test(message)
}

function buildResearchContext(result: FirecrawlSearchResult): {
  promptContext: string
  sources: ResearchSource[]
} {
  const trimmed = result.results.slice(0, 3)

  const sources: ResearchSource[] = trimmed.map((item) => ({
    title: item.title,
    url: item.url,
    snippet: item.snippet,
  }))

  const promptContext = sources
    .map((source, index) => {
      const snippet = source.snippet ? source.snippet.slice(0, 220) : "No snippet provided"
      return `${index + 1}. ${source.title} — ${snippet} (Source: ${source.url})`
    })
    .join("\n")

  return { promptContext, sources }
}

function resolveLocalProviderSpec(
  provider: string | null | undefined,
  baseUrl: string | null | undefined,
  model: string | null | undefined,
  apiKey: string | null | undefined
): LlmProviderSpec | null {
  if (!provider || !baseUrl || !model) {
    return null
  }

  const normalizedProvider = provider.toLowerCase()
  if (normalizedProvider === "ollama") {
    return {
      type: "ollama",
      baseUrl,
      model,
    }
  }

  if (normalizedProvider === "lmstudio") {
    return {
      type: "lmstudio",
      baseUrl,
      model,
      apiKey,
    }
  }

  return null
}

const planner = new BlenderPlanner()
const planExecutor = new PlanExecutor()

async function fetchSceneSummary(): Promise<{ summary: string | null; raw: unknown }> {
  const client = createMcpClient()
  try {
    const response = await client.execute({ type: "get_scene_info" })
    if (response.status === "ok" || response.status === "success") {
      const payload = response.result ?? response.raw ?? null
      return { summary: formatSceneSnapshot(payload), raw: payload }
    }
    return {
      summary: formatSceneSnapshot({ error: response.message ?? "Unknown MCP response" }),
      raw: response,
    }
  } catch (error) {
    return {
      summary: formatSceneSnapshot({ error: error instanceof Error ? error.message : "Failed to fetch scene" }),
      raw: null,
    }
  } finally {
    await client.close().catch(() => undefined)
  }
}

function formatSceneSnapshot(payload: unknown): string | null {
  if (!payload || typeof payload !== "object") {
    return null
  }

  const scene = payload as Record<string, unknown>
  const name = typeof scene.name === "string" ? scene.name : "Unknown"
  const objectCount = typeof scene.object_count === "number" ? scene.object_count : undefined
  const errorMessage = typeof scene.error === "string" ? (scene.error as string) : undefined

  const objectList = Array.isArray(scene.objects) ? scene.objects.slice(0, 12) : []
  const objects = objectList
    .map((raw) => {
      if (!raw || typeof raw !== "object") {
        return "- (unknown object)"
      }
      const obj = raw as Record<string, unknown>
      const identifier = typeof obj.name === "string" ? obj.name : "(unnamed)"
      const type = typeof obj.type === "string" ? obj.type : "UNKNOWN"
      const locationArray = Array.isArray(obj.location) ? obj.location : []
      const location = locationArray
        .slice(0, 3)
        .map((value) => (typeof value === "number" ? value.toFixed(2) : "?"))
        .join(", ")
      return `- ${identifier} [${type}] @ (${location})`
    })
    .join("\n")

  const materials =
    typeof scene.materials_count === "number"
      ? `Materials: ${scene.materials_count}`
      : undefined

  let summary = `Scene: ${name}`
  if (typeof objectCount === "number") {
    summary += ` | Objects: ${objectCount}`
  }
  if (materials) {
    summary += ` | ${materials}`
  }
  if (errorMessage) {
    summary += ` | Error: ${errorMessage}`
  }

  if (objects) {
    summary += `\nObjects:\n${objects}`
  }

  return summary
}


function buildExecutedCommandsFromPlan(
  plan: ExecutionPlan,
  execution: ExecutionResult
): ExecutedCommand[] {
  // Build lookup maps using action string as key (steps don't have stepNumber)
  const completedMap = new Map<string, { step: Record<string, unknown>; result: unknown }>()
  for (const entry of execution.completedSteps) {
    completedMap.set(entry.step.action, entry as { step: Record<string, unknown>; result: unknown })
  }

  const failedMap = new Map<string, string>()
  for (const entry of execution.failedSteps) {
    failedMap.set(entry.step.action, entry.error)
  }

  const commands: ExecutedCommand[] = []
  let failureEncountered = false

  for (const step of plan.steps) {
    const stepAny = step as unknown as Record<string, unknown>
    const stepDesc = (stepAny.expectedOutcome as string)
      || (stepAny.expected_outcome as string)
      || step.rationale
    const completed = completedMap.get(step.action)
    const failedError = failedMap.get(step.action)

    if (completed) {
      commands.push({
        id: createStubId(),
        tool: step.action,
        description: stepDesc,
        status: "executed",
        confidence: 0.65,
        arguments: step.parameters ?? {},
        notes: `Plan rationale: ${step.rationale}`,
        result: completed.result,
      })
      continue
    }

    if (failedError) {
      commands.push({
        id: createStubId(),
        tool: step.action,
        description: stepDesc,
        status: "failed",
        confidence: 0.65,
        arguments: step.parameters ?? {},
        notes: `Plan rationale: ${step.rationale}`,
        error: failedError,
      })
      failureEncountered = true
      continue
    }

    if (failureEncountered) {
      commands.push({
        id: createStubId(),
        tool: step.action,
        description: stepDesc,
        status: "failed",
        confidence: 0.4,
        arguments: step.parameters ?? {},
        notes: `Plan rationale: ${step.rationale}`,
        error: "Step skipped due to earlier failure",
      })
    }
  }

  return commands
}

const chatRequestSchema = z.object({
  projectId: z.string().uuid(),
  conversationId: z.string().uuid().optional(),
  startNew: z.boolean().optional(),
  message: z.string().min(1).max(2000),
  useLocalModel: z.boolean().optional(),
})

async function ensureConversation({
  projectId,
  userId,
  conversationId,
  startNew,
}: {
  projectId: string
  userId: string
  conversationId?: string
  startNew?: boolean
}) {
  if (conversationId) {
    const conversation = await prisma.conversation.findFirst({
      where: {
        id: conversationId,
        project: {
          id: projectId,
          userId,
          isDeleted: false,
        },
      },
      select: { id: true },
    })

    if (!conversation) {
      throw new Error("Conversation not found")
    }

    return conversationId
  }

  if (!startNew) {
    const existing = await prisma.conversation.findFirst({
      where: {
        project: {
          id: projectId,
          userId,
          isDeleted: false,
        },
      },
      orderBy: {
        lastMessageAt: "desc",
      },
      select: { id: true },
    })

    if (existing) {
      return existing.id
    }
  }

  const conversation = await prisma.conversation.create({
    data: {
      projectId,
    },
    select: { id: true },
  })

  return conversation.id
}


export async function POST(req: Request) {
  try {
    const session = await auth()

    if (!session?.user) {
      return NextResponse.json({ error: "Unauthorized" }, { status: 401 })
    }

    const body = await req.json()
    const { projectId, conversationId, startNew, message, useLocalModel } =
      chatRequestSchema.parse(body)

    const project = await prisma.project.findFirst({
      where: {
        id: projectId,
        userId: session.user.id,
        isDeleted: false,
      },
      select: {
        id: true,
        allowHyper3dAssets: true,
        allowSketchfabAssets: true,
        allowPolyHavenAssets: true,
        allowWebResearch: true,
      },
    })

    if (!project) {
      return NextResponse.json(
        { error: "Project not found" },
        { status: 404 }
      )
    }

    const subscriptionTier = session.user.subscriptionTier ?? "free"
    const normalizedTier = subscriptionTier.toLowerCase()
    const assetConfig = {
      allowHyper3d: Boolean(project.allowHyper3dAssets),
      allowSketchfab: Boolean(project.allowSketchfabAssets),
      allowPolyHaven: project.allowPolyHavenAssets !== false,
      allowWebResearch:
        (normalizedTier === "starter" || normalizedTier === "pro") &&
        Boolean(project.allowWebResearch),
    }
    const localProviderSpec = resolveLocalProviderSpec(
      session.user.localLlmProvider,
      session.user.localLlmUrl,
      session.user.localLlmModel,
      session.user.localLlmApiKey
    )

    const wantsLocal =
      subscriptionTier === "free" ? true : useLocalModel === true

    let llmProvider: LlmProviderSpec

    if (wantsLocal) {
      if (!localProviderSpec) {
        const messageText =
          subscriptionTier === "free"
            ? "Local LLM configuration is required for free-tier usage. Configure Ollama or LM Studio in Settings."
            : "Local LLM configuration is incomplete. Please provide base URL and model in Settings or disable local usage."
        return NextResponse.json(
          { error: messageText },
          { status: 400 }
        )
      }
      llmProvider = localProviderSpec
    } else {
      llmProvider = { type: "gemini" }
    }

    const chatSystemPrompt = buildSystemPrompt()

    let researchContext: { promptContext: string; sources: ResearchSource[] } | null = null
    if (assetConfig.allowWebResearch && shouldUseWebResearch(message)) {
      const researchResult = await searchFirecrawl(message)
      if (researchResult) {
        researchContext = buildResearchContext(researchResult)
      }
    }

    const quotaCheck = await canConsumeAiRequest(
      session.user.id,
      session.user.subscriptionTier
    )

    if (!quotaCheck.allowed) {
      const limitLabel =
        quotaCheck.limitType === "daily" ? "daily" : "monthly"
      return NextResponse.json(
        {
          error: `AI request limit reached for your ${limitLabel} allotment. Please upgrade your plan or try again later.`,
          usage: quotaCheck.usage,
        },
        { status: 429 }
      )
    }

    let resolvedConversationId: string
    try {
      resolvedConversationId = await ensureConversation({
        projectId,
        userId: session.user.id,
        conversationId,
        startNew,
      })
    } catch {
      return NextResponse.json(
        { error: "Conversation not found" },
        { status: 404 }
      )
    }

    const historyMessages = await prisma.message.findMany({
      where: { conversationId: resolvedConversationId },
      orderBy: { createdAt: "desc" },
      take: Math.max(0, MAX_HISTORY_MESSAGES - 1),
      select: {
        role: true,
        content: true,
      },
    })

    const trimmedHistory: GeminiMessage[] = historyMessages
      .reverse()
      .map((msg) => ({
        role: msg.role === "assistant" ? "assistant" : "user",
        content: msg.content,
      })) as GeminiMessage[]

    const encoder = new TextEncoder()
    const stream = new ReadableStream({
      async start(controller) {
        const send = (data: unknown) => {
          controller.enqueue(
            encoder.encode(`${JSON.stringify(data)}\n`)
          )
        }

        send({ type: "init", conversationId: resolvedConversationId })

        let assistantText = ""
        let tokenUsage: { promptTokens?: number | null; responseTokens?: number | null; totalTokens?: number | null } | undefined

        try {
          for await (const chunk of streamLlmResponse(llmProvider, {
            history: trimmedHistory,
            messages: [
              {
                role: "user",
                content: message,
              },
            ],
            maxOutputTokens: 512,
            systemPrompt: chatSystemPrompt,
          })) {
            if (chunk.textDelta) {
              assistantText += chunk.textDelta
              send({ type: "delta", delta: chunk.textDelta })
            }
            if (chunk.usage) {
              tokenUsage = chunk.usage
            }
          }


          const sceneSnapshotResult = await fetchSceneSummary()

          // Strategy classification: determine procedural vs neural vs hybrid
          const strategyDecision = await classifyStrategy(message, {
            sceneContext: sceneSnapshotResult.summary ?? undefined,
          })
          send({
            type: "agent:strategy_classification",
            timestamp: new Date().toISOString(),
            strategy: strategyDecision.strategy,
            confidence: strategyDecision.confidence,
            reasoning: strategyDecision.reasoning,
            method: strategyDecision.classificationMethod,
          })

          let executedCommands: ExecutedCommand[] = []
          let planningMetadata: PlanningMetadata | null = null
          let executionLogs: ExecutionLogEntry[] | undefined = undefined
          let planResult: PlanGenerationResult | null = null

          // ── Neural/Hybrid → Guided Workflow (human-in-the-loop) ──
          if (strategyDecision.strategy === "neural" || strategyDecision.strategy === "hybrid") {
            try {
              const workflowProposal = await generateWorkflowProposal(
                message,
                strategyDecision.strategy,
                { sceneContext: sceneSnapshotResult.summary ?? undefined }
              )

              // Send the workflow proposal to the UI
              send({
                type: "agent:workflow_proposal",
                timestamp: new Date().toISOString(),
                proposal: workflowProposal,
              })

              // Build a lightweight planningMetadata for the conversation record
              planningMetadata = {
                planSummary: `Workflow proposed: ${workflowProposal.title} (${workflowProposal.steps.length} steps). Awaiting user action on each step.`,
                planSteps: workflowProposal.steps.map((s) => ({
                  stepNumber: s.stepNumber,
                  action: s.recommendedTool === "neural" ? "neural_generate" : s.recommendedTool === "manual" ? "manual" : "execute_code",
                  parameters: {
                    category: s.category,
                    tool: s.recommendedTool,
                    neuralProvider: s.neuralProvider,
                    workflowStepId: s.id,
                  },
                  rationale: s.toolReasoning,
                  expectedOutcome: s.description,
                })),
                rawPlan: JSON.stringify(workflowProposal, null, 2),
                retries: 0,
                executionSuccess: true, // Proposal was successfully generated
                sceneSnapshot: sceneSnapshotResult.summary,
                strategyDecision,
              }
            } catch (workflowError) {
              console.error("Workflow proposal failed, falling back to planner:", workflowError)
              // Fall through to the procedural planner+executor below
            }
          }

          // ── Procedural → Auto-pilot planner + executor (existing flow) ──
          if (!planningMetadata) {

            try {
              planResult = await planner.generatePlan(
                message,
                {
                  sceneSummary: sceneSnapshotResult.summary ?? undefined,
                  allowHyper3dAssets: assetConfig.allowHyper3d,
                  allowSketchfabAssets: assetConfig.allowSketchfab,
                  allowPolyHavenAssets: assetConfig.allowPolyHaven,
                  researchContext: researchContext?.promptContext,
                  strategyDecision,
                },
                llmProvider
              )

              if (planResult && planResult.plan) {
                const executionResult = await planExecutor.executePlan(
                  planResult.plan,
                  message,
                  {
                    ...assetConfig,
                    enableVisualFeedback: true,
                    onStreamEvent: (event) => send(event),
                    strategyDecision,
                  },
                  planResult.analysis,
                  llmProvider
                )
                executionLogs = executionResult.logs
                executedCommands = buildExecutedCommandsFromPlan(planResult.plan, executionResult)
                planningMetadata = {
                  planSummary: planResult.plan.planSummary,
                  planSteps: planResult.plan.steps,
                  rawPlan: planResult.rawResponse,
                  retries: planResult.retries ?? 0,
                  executionSuccess: executionResult.success,
                  errors: planResult.errors,
                  executionLog: executionResult.logs,
                  sceneSnapshot: sceneSnapshotResult.summary,
                  analysis: planResult.analysis,
                  researchSummary: researchContext?.promptContext,
                  researchSources: researchContext?.sources,
                  strategyDecision,
                }

                if (!executionResult.success) {
                  planningMetadata.executionSuccess = false
                }
              } else if (planResult) {
                const previousLogs = executionLogs
                planningMetadata = {
                  planSummary: "Plan generation failed",
                  planSteps: [],
                  rawPlan: planResult.rawResponse,
                  retries: planResult.retries ?? 0,
                  executionSuccess: false,
                  errors: planResult.errors,
                  fallbackUsed: false,
                  executionLog: previousLogs,
                  sceneSnapshot: sceneSnapshotResult.summary,
                  analysis: planResult.analysis,
                  researchSummary: researchContext?.promptContext,
                  researchSources: researchContext?.sources,
                }
                executedCommands = []
              } else {
                throw new Error("Planner returned no result")
              }
            } catch (error) {
              console.error("Planning pipeline error:", error)
              const messageText =
                error instanceof Error ? error.message : "Unknown planning error"
              planningMetadata = planningMetadata ?? {
                planSummary: "Planner error",
                planSteps: [],
                rawPlan: "",
                retries: 0,
                executionSuccess: false,
                errors: [messageText],
                fallbackUsed: false,
                executionLog: executionLogs,
                sceneSnapshot: sceneSnapshotResult.summary,
                analysis: planResult?.analysis,
                researchSummary: researchContext?.promptContext,
                researchSources: researchContext?.sources,
              }
              executedCommands = []
            }
          } // ← closing brace for procedural fallback block



          if (!planningMetadata) {
            planningMetadata = {
              planSummary: "No execution plan generated",
              planSteps: [],
              rawPlan: "",
              retries: 0,
              executionSuccess: false,
              fallbackUsed: false,
              executionLog: executionLogs,
              sceneSnapshot: sceneSnapshotResult.summary,
              analysis: planResult?.analysis,
              researchSummary: researchContext?.promptContext,
              researchSources: researchContext?.sources,
            }
          }

          const failedCommands = executedCommands.filter((command) => command.status === "failed")
          const overallSuccess =
            planningMetadata.executionSuccess ?? failedCommands.length === 0

          await recordExecutionLog({
            timestamp: new Date().toISOString(),
            conversationId: resolvedConversationId,
            userId: session.user.id,
            projectId,
            request: message,
            planSummary: planningMetadata.planSummary,
            planSteps: planningMetadata.planSteps.length,
            success: overallSuccess,
            fallbackUsed: planningMetadata.fallbackUsed ?? false,
            planRetries: planningMetadata.retries,
            failedCommands: failedCommands.map((command) => ({
              id: command.id,
              tool: command.tool,
              error: command.error,
            })),
            commandCount: executedCommands.length,
            planErrors: planningMetadata.errors,
            tokenUsage,
            executionLog: planningMetadata.executionLog,
            sceneSummary: planningMetadata.sceneSnapshot ?? sceneSnapshotResult.summary,
            researchSummary: planningMetadata.researchSummary,
          })


          const result = await prisma.$transaction(async (tx) => {
            const userMessageRecord = await tx.message.create({
              data: {
                conversationId: resolvedConversationId,
                role: "user",
                content: message,
              },
              select: {
                id: true,
                role: true,
                content: true,
                createdAt: true,
              },
            })

            const assistantMessageRecord = await tx.message.create({
              data: {
                conversationId: resolvedConversationId,
                role: "assistant",
                content: assistantText,
                mcpCommands: executedCommands as unknown as Prisma.InputJsonValue,
                mcpResults: {
                  tokens: tokenUsage,
                  plan: planningMetadata ?? undefined,
                  commands: executedCommands.map((command) => ({
                    id: command.id,
                    tool: command.tool,
                    status: command.status,
                    result: command.result,
                    error: command.error,
                  })),
                } as unknown as Prisma.InputJsonValue,
              },
              select: {
                id: true,
                role: true,
                content: true,
                mcpCommands: true,
                createdAt: true,
              },
            })

            await tx.conversation.update({
              where: { id: resolvedConversationId },
              data: { lastMessageAt: assistantMessageRecord.createdAt },
            })

            return { userMessageRecord, assistantMessageRecord }
          })

          await logUsage({
            userId: session.user.id,
            projectId,
            requestType: "ai_request",
            tokensUsed: tokenUsage?.totalTokens ?? undefined,
          })

          const usage = await getUsageSummary(
            session.user.id,
            session.user.subscriptionTier
          )

          send({
            type: "complete",
            conversationId: resolvedConversationId,
            messages: [result.userMessageRecord, result.assistantMessageRecord],
            usage,
            tokenUsage,
            commandSuggestions: executedCommands,
            planning: planningMetadata,
          })
        } catch (error) {
          console.error("AI chat stream error:", error)
          send({
            type: "error",
            error:
              error instanceof Error
                ? error.message
                : "Failed to process AI request",
          })
        } finally {
          controller.close()
        }
      },
    })

    return new Response(stream, {
      headers: {
        "Content-Type": "application/x-ndjson",
        "Cache-Control": "no-cache",
      },
    })
  } catch (error) {
    console.error("AI chat error:", error)
    if (error instanceof z.ZodError) {
      return NextResponse.json(
        { error: "Invalid input data", details: error.flatten() },
        { status: 400 }
      )
    }

    return NextResponse.json(
      { error: error instanceof Error ? error.message : "Failed to process AI request" },
      { status: 500 }
    )
  }
}
